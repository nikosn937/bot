import streamlit as st
import pandas as pd

# --------------------------------------------------------------------------------
# 1. Î’ÎŸÎ—Î˜Î—Î¤Î™ÎšÎ•Î£ Î£Î¥ÎÎ‘Î¡Î¤Î—Î£Î•Î™Î£ (Î‘Î¦Î‘Î™Î¡Î•Î£Î— Î¤ÎŸÎÎ©Î ÎšÎ‘Î™ TAGGING)
# --------------------------------------------------------------------------------

TONES_MAP = str.maketrans("Î¬Î­Î®Î¯ÏŒÏÏ", "Î±ÎµÎ·Î¹Î¿Ï…Ï")

def normalize_text(text):
    """ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÎµ Ï€ÎµÎ¶Î¬, Î±Ï†Î±Î¹ÏÎµÎ¯ Ï„Î± ÎºÎµÎ½Î¬ ÎºÎ±Î¹ Ï„Î¿Ï…Ï‚ Ï„ÏŒÎ½Î¿Ï…Ï‚."""
    if pd.isna(text):
        return ''
    normalized = str(text).lower().strip()
    return normalized.translate(TONES_MAP)

def get_tags_from_keyword(keyword):
    """Î”Î¹Î±Ï‡Ï‰ÏÎ¯Î¶ÎµÎ¹ Î¼Î¹Î± Ï†ÏÎ¬ÏƒÎ·-ÎºÎ»ÎµÎ¹Î´Î¯ ÏƒÎµ Î¼ÎµÎ¼Î¿Î½Ï‰Î¼Î­Î½Î±, Î¿Î¼Î±Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± tags."""
    if not keyword or pd.isna(keyword):
        return []
    # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ normalize_text ÎºÎ±Î¹ Î¼ÎµÏ„Î¬ ÏƒÏ€Î¬ÎµÎ¹ ÏƒÎµ Î»Î­Î¾ÎµÎ¹Ï‚
    return [normalize_text(word) for word in str(keyword).split() if word]


# --------------------------------------------------------------------------------
# 2. Î¦ÎŸÎ¡Î¤Î©Î£Î— ÎšÎ‘Î™ Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘ Î”Î•Î”ÎŸÎœÎ•ÎÎ©Î Î‘Î ÎŸ CSV (TAGGING LOGIC)
# --------------------------------------------------------------------------------

DATA_FILE = 'class_data.csv' 
DATE_FORMAT = '%d/%m/%Y' 

try:
    df = pd.read_csv(DATA_FILE, encoding='utf-8')
    df.columns = df.columns.str.strip()
    
    required_cols = ['Keyword', 'Info', 'URL', 'Type', 'Date']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"Î£Ï†Î¬Î»Î¼Î±: Î¤Î¿ CSV Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¹Ï‚ Î±ÎºÏÎ¹Î²ÎµÎ¯Ï‚ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚: {', '.join(required_cols)}.")
    
    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î·Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ Date ÎºÎ±Î¹ Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·
    df['Date'] = pd.to_datetime(df['Date'], format=DATE_FORMAT, errors='coerce')
    df_sorted = df.sort_values(by=['Keyword', 'Date'], ascending=[True, False])
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï‡Î¬ÏÏ„Î· Tags Ï€ÏÎ¿Ï‚ ÎšÎ±Ï„Î±Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ (Data Source Dictionary)
    # Î˜Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ Tag (Ï€.Ï‡. 'ÎµÎºÎ´ÏÎ¿Î¼Î·') Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ ÏƒÎµÎ¹ÏÎ­Ï‚ (rows)
    
    # 1. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î¼Î¹Î± Î¼Î¿Î½Î±Î´Î¹ÎºÎ® Î»Î¯ÏƒÏ„Î± Î¼Îµ ÏŒÎ»Î± Ï„Î± keywords (Ï‰Ï‚ strings)
    unique_keywords = df_sorted['Keyword'].unique()
    
    # 2. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± Î»ÎµÎ¾Î¹ÎºÏŒ ÏŒÏ€Î¿Ï… Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ Keyword (Ï€.Ï‡. 'ÏƒÏ‡Î¿Î»Î¹ÎºÎ· ÎµÎºÎ´ÏÎ¿Î¼Î·')
    # ÎºÎ±Î¹ Î· Ï„Î¹Î¼Î® ÎµÎ¯Î½Î±Î¹ Î· Î»Î¯ÏƒÏ„Î± Î¼Îµ ÏŒÎ»Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î·Ï‚ Î³ÏÎ±Î¼Î¼Î®Ï‚
    keyword_to_data_map = df_sorted.groupby('Keyword').apply(
        lambda x: list(zip(x['Info'], x['URL'], x['Type'], x['Date']))
    ).to_dict()

    # 3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Ï„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ Î»ÎµÎ¾Î¹ÎºÏŒ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚: Tag -> [Keyword1, Keyword2, ...]
    tag_to_keyword_map = {}
    for keyword in unique_keywords:
        normalized_tags = get_tags_from_keyword(keyword)
        for tag in normalized_tags:
            if tag not in tag_to_keyword_map:
                tag_to_keyword_map[tag] = set() # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ set Î³Î¹Î± Î½Î± Î±Ï€Î¿Ï†ÏÎ³Î¿Ï…Î¼Îµ Î´Î¹Ï€Î»Î¿ÎºÎ±Ï„Î±Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚
            tag_to_keyword_map[tag].add(keyword)
            
    available_keys_display = sorted(unique_keywords)
    
except Exception as e:
    st.error(f"ÎšÏÎ¯ÏƒÎ¹Î¼Î¿ Î£Ï†Î¬Î»Î¼Î± Î¦ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {e}")
    tag_to_keyword_map = {}
    keyword_to_data_map = {}
    available_keys_display = []

# --------------------------------------------------------------------------------
# 3. UI / Î›ÎŸÎ“Î™ÎšÎ— Î‘ÎÎ‘Î–Î—Î¤Î—Î£Î—Î£
# --------------------------------------------------------------------------------

st.set_page_config(page_title="Î’Î¿Î·Î¸ÏŒÏ‚ Î¤Î¬Î¾Î·Ï‚ (ÎœÎµÏÎ¹ÎºÎ® Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ·)", layout="centered")
st.title("ğŸ¤– Î¨Î·Ï†Î¹Î±ÎºÏŒÏ‚ Î’Î¿Î·Î¸ÏŒÏ‚ Î¤Î¬Î¾Î·Ï‚ (Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î›Î­Î¾ÎµÎ¹Ï‚-ÎšÎ»ÎµÎ¹Î´Î¹Î¬)")
st.markdown("---")

info_message = f"Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚-ÎºÎ»ÎµÎ¹Î´Î¹Î¬: **{', '.join(available_keys_display)}**"
st.info(info_message)

user_input = st.text_input(
    'Î¤Î¹ Î¸Î­Î»ÎµÎ¹Ï‚ Î½Î± Î¼Î¬Î¸ÎµÎ¹Ï‚;', 
    placeholder='Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï€.Ï‡. ÎµÎºÎ´ÏÎ¿Î¼Î·, ÎµÏÎ³Î±ÏƒÎ¹Î±, Î²Î¹Î²Î»Î¹Î±...'
)

if user_input:
    # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ tag Î±Ï€ÏŒ Ï„Î·Î½ ÎµÎ¯ÏƒÎ¿Î´Î¿ Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î· (Ï€.Ï‡. 'ÎµÎºÎ´ÏÎ¿Î¼Î·')
    search_tag = normalize_text(user_input)
    
    # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ ÏŒÎ»Î± Ï„Î± Keywords Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î±Ï…Ï„ÏŒ Ï„Î¿ tag (Ï€.Ï‡. {'ÏƒÏ‡Î¿Î»Î¹ÎºÎ· ÎµÎºÎ´ÏÎ¿Î¼Î·', 'ÎµÎºÎ´ÏÎ¿Î¼Î· ÎµÎ½Î·Î¼ÎµÏÏ‰ÏƒÎµÎ¹Ï‚'})
    matching_keywords = tag_to_keyword_map.get(search_tag, set())
    
    if matching_keywords:
        # Î£Ï…Î»Î»Î­Î³Î¿Ï…Î¼Îµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒ ÏŒÎ»Î± Ï„Î± Keywords Ï€Î¿Ï… Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½
        all_results = []
        for keyword in matching_keywords:
             # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ ÎºÎ¬Î¸Îµ tuple (Info, URL, Type, Date)
            all_results.extend(keyword_to_data_map.get(keyword, [])) 

        st.success(f"Î’ÏÎ­Î¸Î·ÎºÎ±Î½ **{len(all_results)}** Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ **{len(matching_keywords)}** Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚-ÎºÎ»ÎµÎ¹Î´Î¹Î¬ Î³Î¹Î± Ï„Î¿ Î¸Î­Î¼Î±: **{user_input}**")

        # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ (Î¿ ÎºÏÎ´Î¹ÎºÎ±Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·Ï‚ Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ Î¯Î´Î¹Î¿Ï‚)
        for i, (info, url, item_type, date_obj) in enumerate(all_results, 1):
            
            date_str = date_obj.strftime(DATE_FORMAT) if pd.notna(date_obj) else "Î†Î³Î½Ï‰ÏƒÏ„Î· Î—Î¼/Î½Î¯Î±"
            header = f"**ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ· {i}** (Î—Î¼/Î½Î¯Î±: {date_str})"
            
            if item_type.strip().lower() == 'file':
                link_description = info.strip()
                link_url = url.strip()
                if link_url:
                    st.markdown(f"{header}: ğŸ“‚ [{link_description}](<{link_url}>)")
                else:
                    st.markdown(f"{header}: ğŸ’¬ **Î ÏÎ¿ÏƒÎ¿Ï‡Î®:** ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Î¿Ï… Ï‡Ï‰ÏÎ¯Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ¼Î¿. Î ÎµÏÎ¹Î³ÏÎ±Ï†Î®: {link_description}")
            
            elif item_type.strip().lower() == 'text':
                st.markdown(f"{header}: ğŸ’¬ {info}")
            
            else:
                st.markdown(f"{header}: Î†Î³Î½Ï‰ÏƒÏ„Î¿Ï‚ Î¤ÏÏ€Î¿Ï‚ ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ·Ï‚. {info}")
                
    else:
        st.warning(
            f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Î³Î¹Î± Ï„Î¿: '{user_input}'. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¼Î¹Î± Î»Î­Î¾Î· Î±Ï€ÏŒ Ï„Î¹Ï‚ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚-ÎºÎ»ÎµÎ¹Î´Î¹Î¬: **{', '.join(available_keys_display)}**."
        )

st.markdown("---")
st.caption("Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ Ï€Î»Î­Î¿Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î¼ÎµÏÎ¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚-ÎºÎ»ÎµÎ¹Î´Î¹Î¬.")
